{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:19.658171200Z",
     "start_time": "2025-03-17T02:22:19.655147Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:21.310472400Z",
     "start_time": "2025-03-17T02:22:19.666149300Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from numpy import array\n",
    "from random import random\n",
    "from sklearn import metrics\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB#57\n",
    "from sklearn.naive_bayes import GaussianNB#52\n",
    "from sklearn.naive_bayes import MultinomialNB#56\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:21.321380200Z",
     "start_time": "2025-03-17T02:22:21.313897500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提取指定文件 Label 列中的唯一值并排序\n",
    "def target_name(name):\n",
    "    df = pd.read_csv(name,usecols=[\"Label\"])\n",
    "    target_names=sorted(list(df[\"Label\"].unique()))\n",
    "    return target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:21.321380200Z",
     "start_time": "2025-03-17T02:22:21.318385100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:21.333928Z",
     "start_time": "2025-03-17T02:22:21.323809100Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义了一个字典 ml_list，其中包含了多种机器学习模型的实例化对象。\n",
    "# 代码中使用了 scikit-learn 库中的多个分类模型，并设置了不同的超参数。\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ml_list={\"NB\": OneVsRestClassifier(CategoricalNB(alpha=1e-09)),\n",
    "    \"DT\": OneVsRestClassifier(DecisionTreeClassifier(criterion='gini', max_depth=26,\n",
    "                       max_features=26, min_samples_split=6)),\n",
    "    \"RF\": OneVsRestClassifier(RandomForestClassifier(bootstrap=True, criterion=\"gini\", max_depth=18, max_features=8, min_samples_split=9, n_estimators=96)),}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation Algorithm notmal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:21.358969600Z",
     "start_time": "2025-03-17T02:22:21.340179600Z"
    }
   },
   "outputs": [],
   "source": [
    "altime=0\n",
    "#def most_frequent(List): \n",
    "#    return max(set(List), key = List.count) \n",
    "\n",
    "\n",
    "# 找出列表中出现频率最高的元素。如果有多个元素具有相同的最高频率，则随机返回其中一个\n",
    "def most_frequent(List):\n",
    "    occurence_count = Counter(List)\n",
    "    occurence_count={k: v for k, v in sorted(occurence_count.items(), key=lambda item: item[1],reverse=True)}\n",
    "    big=list(occurence_count.values())\n",
    "    big=big.count(big[0])\n",
    "    return list(occurence_count.keys())[np.random.randint(big)]\n",
    "\n",
    "# 将列表 a 分成 n 个大致相等的部分\n",
    "def split(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "# 找出 df 中具有重复 \"dominant MAC\" 的异常值，并返回这些异常值的列表\n",
    "def create_exception(df): \n",
    "    exception_list=[]\n",
    "    dominant_mac=[]\n",
    "    for i in df['aggregated'].unique():\n",
    "        k=df[df['aggregated']==i]\n",
    "        for ii in ['MAC']:\n",
    "            hist = {}\n",
    "            for x in k[ii].values:\n",
    "                hist[x] = hist.get(x, 0) + 1\n",
    "            hist=dict(sorted(hist.items(), key=lambda item: item[1],reverse=True))\n",
    "            temp=next(iter(hist))\n",
    "            if temp not in dominant_mac:\n",
    "                dominant_mac.append(temp)\n",
    "            else:\n",
    "                exception_list.append(temp)\n",
    "    return exception_list\n",
    "\n",
    "\n",
    "\n",
    "# 将 m_test 和 predict 合并，生成一个新的分类结果 aggregated。如果 mixed 为 True，则使用混合方法处理异常值\n",
    "def merged(m_test,predict,step,mixed):\n",
    "    second=time.time()\n",
    "    mac_test=[]\n",
    "    for q in m_test.index:\n",
    "        mac_test.append(m_test[q])\n",
    "\n",
    "    d_list=sorted(list(m_test.unique()))\n",
    "    devices={}\n",
    "    for q in d_list:\n",
    "        devices[q]=[]    \n",
    "\n",
    "\n",
    "    new_y=[0]*len(m_test)\n",
    "\n",
    "    for q,qq in enumerate (mac_test):\n",
    "        devices[qq].append(q)\n",
    "    for q in devices:\n",
    "        a = [devices[q][j:j + step] for j in range(0, len(devices[q]), step)]  \n",
    "        for qq in a:\n",
    "            step_list=[]\n",
    "            for qqq in qq:\n",
    "                step_list.append(predict[qqq])\n",
    "            add=most_frequent(list(step_list))\n",
    "            for qqq in qq:\n",
    "                new_y[qqq]=add\n",
    "    results=pd.DataFrame(m_test)\n",
    "    results[\"aggregated\"]=new_y\n",
    "    results[\"normal\"]=predict\n",
    "    \n",
    "    #MIXED METHOD\n",
    "    if mixed:\n",
    "        exception=create_exception(results)\n",
    "        for q in exception:\n",
    "            results.loc[results.MAC == q, 'aggregated'] = results['normal']\n",
    "\n",
    "    return results[\"aggregated\"].values,time.time()-second\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:21.372799300Z",
     "start_time": "2025-03-17T02:22:21.357973700Z"
    }
   },
   "outputs": [],
   "source": [
    "# 计算和记录分类模型的性能指标，包括准确率（accuracy）、召回率（recall）、精确率（precision）、\n",
    "# F1 分数（f1-score）、平衡准确率（balanced accuracy）、Cohen's Kappa 系数等。\n",
    "# 使用 classification_report 生成分类报告，并累加存储为 DataFrame\n",
    "def score(altime,train_time,test_time,predict,y_test,class_based_results,i,cv,dname,ii):\n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    accuracy=[]\n",
    "    total_time=[]\n",
    "    kappa=[]\n",
    "    accuracy_b=[]\n",
    "    \n",
    "    rc=sklearn.metrics.recall_score(y_test, predict,average= \"macro\")\n",
    "    pr=sklearn.metrics.precision_score(y_test, predict,average= \"macro\")\n",
    "    f_1=sklearn.metrics.f1_score(y_test, predict,average= \"macro\")        \n",
    "    report = classification_report(y_test, predict, target_names=target_names,output_dict=True)\n",
    "    cr = pd.DataFrame(report).transpose()\n",
    "    if class_based_results.empty:\n",
    "        class_based_results =cr\n",
    "    else:\n",
    "        class_based_results = class_based_results.add(cr, fill_value=0)\n",
    "    precision.append(float(pr))\n",
    "    recall.append(float(rc))\n",
    "    f1.append(float(f_1))\n",
    "    accuracy_b.append(balanced_accuracy_score( y_test,predict))\n",
    "    accuracy.append(accuracy_score(y_test, predict))\n",
    "\n",
    "    kappa.append(round(float(sklearn.metrics.cohen_kappa_score(y_test, predict, \n",
    "    labels=None, weights=None, sample_weight=None)),15))\n",
    "    print ('%-15s %-3s %-3s %-6s  %-5s %-5s %-5s %-5s %-8s %-5s %-8s %-8s%-8s%-8s' % (dname,i,cv,ii[0:6],str(round(np.mean(accuracy),2)),str(round(np.mean(accuracy_b),2)),\n",
    "        str(round(np.mean(precision),2)), str(round(np.mean(recall),2)),str(round(np.mean(f1),4)), \n",
    "        str(round(np.mean(kappa),2)),str(round(np.mean(train_time),2)),str(round(np.mean(test_time),2)),str(round(np.mean(test_time)+np.mean(train_time),2)),str(round(np.mean(altime),2))))\n",
    "    lines=(str(dname)+\",\"+str(i)+\",\"+str(cv)+\",\"+str(ii)+\",\"+str(round(np.mean(accuracy),15))+\",\"+str(round(np.mean(accuracy_b),15))+\",\"+str(round(np.mean(precision),15))+\",\"+ str(round(np.mean(recall),15))+\",\"+str(round(np.mean(f1),15))+\",\"+str(round(np.mean(kappa),15))+\",\"+str(round(np.mean(train_time),15))+\",\"+str(round(np.mean(test_time),15))+\",\"+str(altime)+\"\\n\")\n",
    "    return lines,class_based_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:22.382270500Z",
     "start_time": "2025-03-17T02:22:21.384197900Z"
    }
   },
   "outputs": [],
   "source": [
    "# loop1, loop2: 训练数据和测试数据的文件路径。\n",
    "# output_csv: 用于记录结果的 CSV 文件路径。\n",
    "# cols: 需要读取的列。\n",
    "# step: 分组的大小（用于 merged 函数）。\n",
    "# mixed: 是否使用混合方法（用于 merged 函数）。\n",
    "# dname: 数据集名称。\n",
    "import nltk  # 导入 nltk 以计算编辑距离\n",
    "import Levenshtein\n",
    "def ML(loop1,loop2,output_csv,cols,step,mixed,dname):\n",
    "\n",
    "    ths = open(output_csv, \"w\")\n",
    "    ths.write(\"Dataset,T,CV,ML algorithm,Acc,b_Acc,Precision, Recall , F1-score, kappa ,tra-Time,test-Time,Al-Time\\n\")\n",
    "    \n",
    "\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    \n",
    "    for ii in ml_list:\n",
    "        print ('%-15s %-3s %-3s %-6s  %-5s %-5s %-5s %-5s %-8s %-5s %-8s %-8s%-8s%-8s'%\n",
    "               (\"Dataset\",\"T\",\"CV\",\"ML alg\",\"Acc\",\"b_Acc\",\"Prec\", \"Rec\" , \"F1\", \"kap\" ,\"tra-T\",\"test-T\",\"total\",\"al-time\"))\n",
    "        class_based_results=pd.DataFrame()#\"\" #pd.DataFrame(0, index=np.arange((len(target_names)+3)), columns=[\"f1-score\",\"precision\",\"recall\",\"support\"])\n",
    "        cm=pd.DataFrame()\n",
    "        cv=0\n",
    "        if ii in [\"GB\",\"SVM\"]: #for slow algorithms.\n",
    "            repetition=3\n",
    "        else:\n",
    "            repetition=3\n",
    "        if ii in [\"MLP\"]: #for slow algorithms.\n",
    "            repetition=1\n",
    "\n",
    "        for i in range(repetition):\n",
    "\n",
    "\n",
    "\n",
    "            #TRAIN\n",
    "            df = pd.read_csv(loop1,usecols=cols)\n",
    "            try:df=df.replace({\"Protocol\": Protocol})\n",
    "            except:pass\n",
    "            m_train=df[\"MAC\"]\n",
    "            del df[\"MAC\"]\n",
    "            X_train =df[df.columns[0:-2]]\n",
    "            X_train=np.array(X_train)\n",
    "            df[df.columns[-1]] = df[df.columns[-1]].astype('category')\n",
    "            y_train=df[df.columns[-1]].cat.codes  \n",
    "            \n",
    "            # 提取训练样本的 nilsimsa_hash 并按类别存储\n",
    "            train_nilsimsa = df[df.columns[-2]]  # nilsimsa_hash 列\n",
    "            hash_dict = {}\n",
    "            for label, hash_value in zip(y_train, train_nilsimsa):\n",
    "                if label not in hash_dict:\n",
    "                    hash_dict[label] = []\n",
    "                hash_dict[label].append(hash_value)\n",
    "\n",
    "            #TEST\n",
    "            df = pd.read_csv(loop2,usecols=cols)\n",
    "            try:df=df.replace({\"Protocol\": Protocol})\n",
    "            except:pass\n",
    "            df = shuffle(df)\n",
    "            m_test=df[\"MAC\"]\n",
    "            del df[\"MAC\"]\n",
    "            X_test =df[df.columns[0:-2]]\n",
    "            X_test=np.array(X_test)\n",
    "            df[df.columns[-1]] = df[df.columns[-1]].astype('category')\n",
    "            y_test=df[df.columns[-1]].cat.codes\n",
    "            \n",
    "            # 提取测试样本的 nilsimsa_hash\n",
    "            test_nilsimsa = df[df.columns[-2]]  # nilsimsa_hash 列\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            results_y=[]\n",
    "            cv+=1\n",
    "            results_y.append(y_test)\n",
    "\n",
    "\n",
    "     \n",
    "   \n",
    "\n",
    "            #machine learning algorithm is applied in this section\n",
    "            clf = ml_list[ii]#choose algorithm from ml_list dictionary\n",
    "            second=time.time()\n",
    "            clf.fit(X_train, y_train)\n",
    "            train_time=(float((time.time()-second)) )\n",
    "            second=time.time()\n",
    "            probabilities = clf.predict_proba(X_test)  # 获取概率矩阵\n",
    "            test_time=(float((time.time()-second)) )\n",
    "            \n",
    "            # 自定义预测逻辑\n",
    "            predict = []\n",
    "            threshold = 0.001  # 阈值\n",
    "            for sample_idx in range(X_test.shape[0]):\n",
    "                prob = probabilities[sample_idx]\n",
    "                top_classes = np.argsort(prob)[::-1]  # 从高到低排序\n",
    "                max_class = top_classes[0]  # 概率最高的类别\n",
    "                max_prob = prob[max_class]  # 最高概率值\n",
    "            \n",
    "                # 初始化变量\n",
    "                chosen_class = max_class  # 默认选择概率最高的类别\n",
    "                min_distance = float('inf')  # 初始化最小编辑距离为无穷大\n",
    "                test_hash = test_nilsimsa.iloc[sample_idx]  # 当前测试样本的 nilsimsa_hash\n",
    "            \n",
    "                # 遍历所有类别，找出与最高概率的差值小于阈值的类别\n",
    "                for class_idx in top_classes:\n",
    "                    if max_prob - prob[class_idx] < threshold:\n",
    "                        # 当前类别的 nilsimsa_hash 列表\n",
    "                        class_hashes = hash_dict[class_idx]\n",
    "                        if class_hashes:\n",
    "                            # 计算当前类别的最小编辑距离\n",
    "                            min_distance_current = min(Levenshtein.distance(test_hash, h) for h in class_hashes)\n",
    "                            # 如果当前类别的最小编辑距离小于已记录的最小编辑距离，更新选择\n",
    "                            if min_distance_current < min_distance:\n",
    "                                min_distance = min_distance_current\n",
    "                                chosen_class = class_idx\n",
    "                    else:\n",
    "                        # 如果当前类别的概率差已经大于阈值，停止进一步检查\n",
    "                        break\n",
    "            \n",
    "                predict.append(chosen_class)\n",
    "            \n",
    "            predict = np.array(predict)\n",
    "            \n",
    "            if step==1:\n",
    "                altime=0\n",
    "                lines,class_based_results=score(altime,train_time,test_time,predict,y_test,class_based_results,i,cv,dname,ii)\n",
    "            else:\n",
    "                predict,altime=merged(m_test,predict,step,mixed)\n",
    "                lines,class_based_results=score(altime,train_time,test_time,predict,y_test,class_based_results,i,cv,dname,ii)\n",
    "            ths.write (lines)\n",
    "\n",
    "\n",
    "            df_cm = pd.DataFrame(confusion_matrix(y_test, predict))\n",
    "            # print(df_cm)\n",
    "            \n",
    "\n",
    "            # 绘制混淆矩阵\n",
    "            plt.figure(figsize=(10, 8))  # 设置图片大小\n",
    "            sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5, cbar=True, xticklabels=target_names, yticklabels=target_names)\n",
    "            \n",
    "            # 添加标题和标签\n",
    "            plt.title(f\"Confusion Matrix - {ii} (CV={cv}, Rep={i})\")\n",
    "            plt.xlabel(\"Predicted Labels\")\n",
    "            plt.ylabel(\"True Labels\")\n",
    "            \n",
    "            # 保存为图片\n",
    "            confusion_matrix_image = f\"./Aalto_test/confusion_matrix_{ii}_cv{cv}_rep{i}.png\"\n",
    "            plt.savefig(confusion_matrix_image)\n",
    "            plt.close()  # 关闭图片窗口，避免内存泄漏\n",
    "            \n",
    "            if cm.empty:\n",
    "                cm =df_cm\n",
    "            else:\n",
    "                cm = cm.add(df_cm, fill_value=0)\n",
    "            \n",
    "        class_based_results=class_based_results/repetition\n",
    "        #print(class_based_results)\n",
    "        class_based_results.to_csv(\"class_based_results.csv\")\n",
    "        if False:\n",
    "            cm=cm//repetition\n",
    "            graph_name=output_csv+ii+\"_confusion matrix.pdf\"   \n",
    "            plt.figure(figsize = (40,28))\n",
    "            sns.heatmap(cm,xticklabels=target_names, yticklabels=target_names, annot=True, fmt='g')\n",
    "            plt.savefig(graph_name,bbox_inches='tight')#, dpi=400)\n",
    "            plt.show()\n",
    "            #print(cm)\n",
    "            print(\"\\n\\n\\n\")             \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    ths.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aalto Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:22:22.382270500Z",
     "start_time": "2025-03-17T02:22:22.373902600Z"
    }
   },
   "outputs": [],
   "source": [
    "feature= ['pck_size', 'Ether_type', 'LLC_ctrl', 'EAPOL_version', 'EAPOL_type', 'IP_ihl', 'IP_tos', 'IP_len', 'IP_flags', 'IP_DF', 'IP_ttl', 'IP_options', 'ICMP_code', 'TCP_dataofs', 'TCP_FIN', 'TCP_ACK', 'TCP_window', 'UDP_len', 'DHCP_options', 'BOOTP_hlen', 'BOOTP_flags', 'BOOTP_sname', 'BOOTP_file', 'BOOTP_options', 'DNS_qr', 'DNS_rd', 'DNS_qdcount', 'dport_class', 'payload_bytes', 'entropy', 'nilsimsa_hash',\n",
    "\"MAC\",\n",
    "'Label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT & NB & RF &  KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T02:29:56.658127300Z",
     "start_time": "2025-03-17T02:22:22.381176100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset         T   CV  ML alg  Acc   b_Acc Prec  Rec   F1       kap   tra-T    test-T  total   al-time \n",
      "Aalto_test_1    0   1   NB      0.62  0.63  0.59  0.63  0.5627   0.59  1.55     0.42    1.97    0.0     \n",
      "Aalto_test_1    1   2   NB      0.62  0.63  0.59  0.63  0.5627   0.59  1.33     0.33    1.65    0.0     \n",
      "Aalto_test_1    2   3   NB      0.62  0.63  0.59  0.63  0.5627   0.59  1.33     0.33    1.66    0.0     \n",
      "Dataset         T   CV  ML alg  Acc   b_Acc Prec  Rec   F1       kap   tra-T    test-T  total   al-time \n",
      "Aalto_test_1    0   1   DT      0.73  0.74  0.78  0.74  0.751    0.71  2.32     0.07    2.39    0.0     \n",
      "Aalto_test_1    1   2   DT      0.73  0.74  0.78  0.74  0.7509   0.71  2.31     0.06    2.37    0.0     \n",
      "Aalto_test_1    2   3   DT      0.73  0.74  0.79  0.74  0.7525   0.71  2.34     0.06    2.4     0.0     \n",
      "Dataset         T   CV  ML alg  Acc   b_Acc Prec  Rec   F1       kap   tra-T    test-T  total   al-time \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m output_csv\u001B[38;5;241m=\u001B[39mdataset\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(sayac)\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(step)\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(mixed)\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     12\u001B[0m target_names\u001B[38;5;241m=\u001B[39mtarget_name(test)\n\u001B[1;32m---> 13\u001B[0m ML(train,test,output_csv,feature,step,mixed,dataset[\u001B[38;5;241m2\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;28mstr\u001B[39m(step))\n",
      "Cell \u001B[1;32mIn[7], line 110\u001B[0m, in \u001B[0;36mML\u001B[1;34m(loop1, loop2, output_csv, cols, step, mixed, dname)\u001B[0m\n\u001B[0;32m    107\u001B[0m class_hashes \u001B[38;5;241m=\u001B[39m hash_dict[class_idx]\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m class_hashes:\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;66;03m# 计算当前类别的最小编辑距离\u001B[39;00m\n\u001B[1;32m--> 110\u001B[0m     min_distance_current \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(Levenshtein\u001B[38;5;241m.\u001B[39mdistance(test_hash, h) \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m class_hashes)\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;66;03m# 如果当前类别的最小编辑距离小于已记录的最小编辑距离，更新选择\u001B[39;00m\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m min_distance_current \u001B[38;5;241m<\u001B[39m min_distance:\n",
      "Cell \u001B[1;32mIn[7], line 110\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    107\u001B[0m class_hashes \u001B[38;5;241m=\u001B[39m hash_dict[class_idx]\n\u001B[0;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m class_hashes:\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;66;03m# 计算当前类别的最小编辑距离\u001B[39;00m\n\u001B[1;32m--> 110\u001B[0m     min_distance_current \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(Levenshtein\u001B[38;5;241m.\u001B[39mdistance(test_hash, h) \u001B[38;5;28;01mfor\u001B[39;00m h \u001B[38;5;129;01min\u001B[39;00m class_hashes)\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;66;03m# 如果当前类别的最小编辑距离小于已记录的最小编辑距离，更新选择\u001B[39;00m\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m min_distance_current \u001B[38;5;241m<\u001B[39m min_distance:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "test='Aalto_test_IoTDevID_nilsimsa.csv'\n",
    "train='Aalto_BIG_train_IoTDevID_nilsimsa.csv'\n",
    "\n",
    "\n",
    "dataset=\"./Aalto_test/\"\n",
    "step=1\n",
    "\n",
    "\n",
    "mixed=False\n",
    "sayac=1\n",
    "output_csv=dataset+str(sayac)+\"_\"+str(step)+\"_\"+str(mixed)+\".csv\"\n",
    "target_names=target_name(test)\n",
    "ML(train,test,output_csv,feature,step,mixed,dataset[2:-1]+\"_\"+str(step))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM & GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:29:56.661225500Z",
     "start_time": "2025-03-17T02:29:56.659124800Z"
    }
   },
   "outputs": [],
   "source": [
    "# ml_list={\"SVM\":SVC(C=10,gamma=1),\n",
    "#          \"GB\":GradientBoostingClassifier(learning_rate=0.001,subsample=0.1,n_estimators=500,max_depth= 10,)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-17T02:29:56.660222700Z"
    }
   },
   "outputs": [],
   "source": [
    "# test='Aalto_test_IoTDevID.csv'\n",
    "# train='Aalto_BIG_train_IoTDevID.csv'\n",
    "# \n",
    "# \n",
    "# \n",
    "# dataset=\"./Aalto/\"\n",
    "# step=1\n",
    "# \n",
    "# \n",
    "# mixed=False\n",
    "# sayac=2\n",
    "# output_csv=dataset+str(sayac)+\"_\"+str(step)+\"_\"+str(mixed)+\"100_svm_gb.csv\"\n",
    "# target_names=target_name(test)\n",
    "# ML(train,test,output_csv,feature,step,mixed,dataset[2:-1]+\"_\"+str(step))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:29:56.662246800Z",
     "start_time": "2025-03-17T02:29:56.662246800Z"
    }
   },
   "outputs": [],
   "source": [
    "# ml_list={\"GB\":GradientBoostingClassifier(learning_rate=0.001,subsample=0.1,n_estimators=500,max_depth= 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T02:29:56.668927400Z",
     "start_time": "2025-03-17T02:29:56.664361100Z"
    }
   },
   "outputs": [],
   "source": [
    "# test='Aalto_test_IoTDevID.csv'\n",
    "# train='Aalto_BIG_train_IoTDevID.csv'\n",
    "# \n",
    "# \n",
    "# \n",
    "# dataset=\"./Aalto/\"\n",
    "# step=1\n",
    "# \n",
    "# \n",
    "# mixed=False\n",
    "# sayac=2\n",
    "# output_csv=dataset+str(sayac)+\"_\"+str(step)+\"_\"+str(mixed)+\"100_svm_gb_GB.csv\"\n",
    "# target_names=target_name(test)\n",
    "# ML(train,test,output_csv,feature,step,mixed,dataset[2:-1]+\"_\"+str(step))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-17T02:29:56.666644700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# \n",
    "# \n",
    "# \n",
    "# test='Aalto_test_IoTDevID.csv'\n",
    "# train='Aalto_BIG_train_IoTDevID.csv'\n",
    "# \n",
    "# \n",
    "# \n",
    "# dataset=\"./Aalto/\"\n",
    "# step=1\n",
    "# \n",
    "# ml_list={\"MLP\":MLPClassifier(solver= 'adam', learning_rate= 'constant', hidden_layer_sizes= (1220, 1965), alpha= 0.1, activation= 'relu')}\n",
    "# \n",
    "# mixed=False\n",
    "# sayac=2\n",
    "# output_csv=dataset+str(sayac)+\"_\"+str(step)+\"_\"+str(mixed)+\"100_mlp.csv\"\n",
    "# target_names=target_name(test)\n",
    "# ML(train,test,output_csv,feature,step,mixed,dataset[2:-1]+\"_\"+str(step))   \n",
    "# \n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-17T02:29:56.667730800Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
